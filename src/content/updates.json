[
  {
    "title": "Nibble",
    "subtitle": "What I Did and What I Learned",
    "anchor": "nibble-update",
    "date": "October 12, 2020",
    "body": "![Nibble logo](nibble.png \"The iconic Nibble logo\")In July of 2020, I came across an amazing opportunity: working as the CTO for a nascent startup called [Nibble](https://nibble.club). Nibble's vision was to create a marketplace for food that restaurants would normally throw out, creating a win-win-win situation: restaurants make money where they would otherwise throw it down the drain, customers get cheap and high quality food, and the environment benefits from far less food being wasted. I was hooked on the idea, and I was incredibly excited about the opportunity to build an entire platform from scratch.\n\nAlthough I still was interning at [Salesforce](#salesforce-summer-2) by day, I spent almost all of my remaining free time building Nibble up from scratch. In this update, I'll go over what I did (which I'm quite proud of!) and what I learned from it. This will be a fairly technical update, because my responsibilities were almost entirely technical.\n\n### Planning\n\nMy first task was to make a mockup of the Nibble app. At this point we were imagining just writing a mobile app, so I pulled together a Figma wireframe in a couple of days. Satisfied with this frontend plan (we decided to make it a responsive website, to reach as many customers as possible at once), it was time to plan the backend. My choices were largely influenced by what I was comfortable with from my previous experiences, particularly at Salesforce - we wanted to get a product out as quickly as possible, and therefore I didn't want to waste time learning brand new technologies. So I knew I wanted to use GraphQL to communicate between frontend and backend, and I knew I wanted to build on AWS. As luck would have it, AWS offers a managed GraphQL service, [AWS AppSync](https://aws.amazon.com/appsync/), so I ran with that. Then after selecting database, cache, search, and authentication providers, all that was left was to actually build it!![Nibble architecture diagram](nibble-architecture.png \"The architecture I designed for the Nibble platform\")The first step was to write the GraphQL schema, the contract between the frontend and backend. But once that was done, I worked on the frontend and backend in tandem, never letting one get too far in front of the other. Despite that, I'll talk about them one at a time - it's a bit easier to keep track that way.\n\n### Frontend\n\nThe frontend was done entirely in TypeScript, using React. For one, I had learned React in Summer 2019 for my Salesforce internship, making it a natural choice. For another, React and GraphQL go together like peanut butter and jelly (unsurprising, considering they were both developed at Facebook). Indeed, it was almost shocking how many times the frontend fell into place so smoothly after implementing the backend calls.\n\nThe two most useful lessons I took away from my frontend development experience were the values of reuse, and the usefulness of bottom-up development. First, on the values of reuse. In the very beginning of pulling together the site, I created a common [app theme](https://github.com/nibble-club/nibble/blob/develop/frontend/src/common/theming/theming.ts). This paid off massive dividends throughout the project. For one, it kept the site cohesive, with common spacing, colors, animations, and more. And for another, it made changes to the site layout far more easy, drastically reducing the number of code changes needed. This is a representative example, but it is far from the only example; common and reusable components and functions also saved countless hours.\n\nWhen I started writing the frontend code, it was hard to know where to begin. So I started from the smallest and simplest components, and used [Storybook](https://storybook.js.org/) to test them with various properties. This style of *component-driven development* made the entire process far smoother - I was able to learn lessons about what worked and what didn't quickly, and in isolation, largely preventing any major rabbit holes and wasted effort.\n\nUltimately, the result was a responsive and performant frontend, with both personality and familiarity. And this included an admin portal as well! I'm quite proud of the map views - laying out restaurants on a map and zipping around to highlight the selected restaurant as the user scrolled - but I'm most proud of how professional and smooth the entire thing looked, even though it was just me working on it.![App screenshot](nibble-screenshot.png \"A screenshot of a Nibble user's home page\")![App screenshot](nibble-map-view.png \"The aforementioned map view, showing restaurants closest to the user's location\")\n\n### Backend\n\nWhile I'm quite happy with what I did on the frontend side, I'll be the first to admit there was nothing particularly groundbreaking. But on the backend side, I pulled together a massively scalable, incredibly performant, and simple to manage solution, offering all kinds of features I wouldn't have even believed were possible at the project's outset.\n\nFirst of all, the architecture itself. Nibble is almost entirely serverless - all of the business logic happens in Lambda functions, invoked by the AppSync GraphQL API. This completely eliminates the need to worry about scaling servers, eliminating one of the hardest parts of managing a service. And thanks to AWS's managed database, cache, and search offerings, the entire infrastructure works with practically no intervention. Oh, and it's all quite secure as well - more on that later.\n\nAll the Lambda functions I wrote used Python, and each Lambda function would have a very small area of responsibility, neatly separating concerns and avoiding any scary monoliths. It's almost like the microservices model taken to the extreme - perhaps we could call it a nanoservices model.\n\nOne of the most important things the backend needed to handle were reservations - users need to be able to reserve food easily, and the backend needs to provide certain guarantees on that behavior. For one, if two users attempt to reserve a Nibble when only one is available, then only one of them should get it. The backend handles these race conditions, using Redis to provide a distributed locking solution as a key step.\n\nAnother key feature the backend needed to provide was search, across both restaurants and the nibbles they offer. Thanks to Elasticsearch, I was able to implement search quite easily, allowing filtering by distance and availability windows along with basic keyword searches. But that wasn't all Elasticsearch was useful for; it also helped with the other side of the search coin, recommendations. Using Elasticsearch, I was able to dynamically select and display Nibbles to users every time they logged in, selected based on criteria like distance from the user and price. And while I haven't built it yet, even more personalized suggestions based on past activity would fit cleanly into the current schema. But even without that, I was quite happy with the amount of intelligence I was able to add to the backend.\n\n### Developer Experience\n\nAll this about the frontend and backend is great, but I still haven't talked about a very important piece of the work I did: the DevOps work. Now, you may think this is a little silly - I was the only person working on this, so how much does this really matter? The answer is simple: while it was nice to have for me, the utility would shine more and more as more developers joined the Nibble team.\n\nWhat exactly was this developer experience? In a nutshell, developers could authenticate painlessly, then test their code, deploy it, and update physical AWS and Heroku infrastructure with a single shell command. AWS is an incredible suite of technologies, but it is incredibly complicated; the solutions I wrote almost completely eliminated that complexity. This was largely thanks to [Terraform](https://www.terraform.io/), a revolutionary solution allowing designing and deploying infrastructure to be as simple as writing and running code.\n\nUsing Terraform, not only did the massive AWS-based architecture become easy to set up once, it was trivially repeatable across environments, throughout development, QA, and production.![AWS accounts diagram](nibble-accounts.png \"The various AWS accounts involved - all of which were bootstrapped and fully managed by Terraform\")It also allowed for much more security; resources were created in a VPC whenever possible, and had their communications restricted as strictly as possible. It even made ops tasks like adding AWS IAM users and managing their permissions (usually a massively complicated headache) to be as easy as merging and deploying pull requests. I would say that the code I wrote for the Nibble developer experience is probably the most generally useful of all the Nibble code, and once again I'm proud to have built it.\n\n### Takeaways\n\nUltimately Nibble didn't become everything it could have been, which is definitely a bummer. It was hard to get restaurants to sign up, which is perhaps understandable in the midst of a pandemic. But when it comes to what I was able to accomplish technically, I have no regrets. I learned so much about how the software sausage is made, so to speak; and as I've already said several times, I'm quite proud of what I pulled together. I'm excited to see how these lessons will carry forward into whatever I do next!"
  },
  {
    "title": "Summer 2 at Salesforce",
    "subtitle": "Analytics Built on AWS",
    "anchor": "salesforce-summer-2",
    "date": "September 9, 2020",
    "body": "![Philanthropy Cloud logo](spc-logo.png)After spending summer 2019 in San Francisco interning at Salesforce.org (side note: this was absolutely my dream summer; it's hard to imagine I'll ever have a more relaxing and fun summer in my life!), I was happy to accept a return offer for summer 2020, working with the same [Philanthropy Cloud](https://www.salesforce.org/philanthropy-cloud-overview/) team. I was quite relieved when the internship wasn't cancelled, even as COVID-19 was upending everything in March and April, and while it was virtual (of course) I still had an overwhelmingly positive experience.\n\nThe goal of the project me and one other intern, ZoÃ«, were assigned to was to add real-time analytics to the Philanthropy Cloud platform. At its simplest, we wanted to be able to know when any unusual and notable activity was happening (think a large donation, or an unusually high number of pledges to volunteer at a given event) as it happened. We decided the best way to do this was with Slack notifications, so the UI was simple; the work would come in actually designing and implementing the analytics pipeline.\n\nIn order to dovetail well with existing analytics work, we designed the system on AWS's platform. As it happens, I had never used AWS before, so this turned out to be quite a useful crash course in its offerings (and using Terraform to deploy to AWS, too!). We settled on [Redis](https://redis.io) as our data storage solution: it offers blazing-fast performance thanks to its in-memory design, and crucially offered several useful data structures (particularly sets and sorted sets) which fit our use case perfectly. To handle the actual business logic, we wrote Lambda functions (3 total), which lived in a VPC along with the Redis store. Then to decouple the event producers (the Philanthropy Cloud backend, which exists on Heroku rather than AWS) from the consumer (our pipeline), we used AWS's [Kinesis Data Streams](https://aws.amazon.com/kinesis/data-streams/).\n\nWith those three key components in place, the implementation went smoothly. The Lambda functions separated concerns cleanly, making it easy to split the work on writing them. Writing the Terraform code to describe the necessary infrastructure was my sole responsibility, and despite a somewhat steep learning curve I was able to pull all the different components together (and I only needed to talk to AWS support once!). Also, being interns and therefore wanting to be on our best behavior, we made a point to test our code thoroughly, both with unit tests for all the Lambda functions and end-to-end tests using fake event producers. Once we had our entire pipeline working, the final step was tying it in with the real event producers. This was non-trivial because of the complexity of the Philanthropy Cloud backend. Furthermore, we had to support seeding (allowing our pipeline to make better observations by comparing real-time events to moving averages and past numbers), and so I personally rewrote our event handler to be idempotent. This filled up the remainder of the 12-week internship, and by the time we presented our work to our team we had implemented the entire pipeline!\n\nI was happy with this second summer because not only did I have the chance to join the same team and reconnect with all the coworkers who had been so helpful during my first summer, I was also able to learn a whole new set of technologies and really challenge myself on what I could accomplish. It was strange being virtual, for sure, but ultimately summer 2 at Salesforce was a success."
  }
]